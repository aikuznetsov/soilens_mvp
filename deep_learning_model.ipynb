{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-02T05:23:27.880692Z",
     "start_time": "2025-04-02T05:23:27.509044Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_1_df = pd.read_csv('resources/file_1.csv')\n",
    "file_2_df = pd.read_csv('resources/file_2.csv')\n",
    "file_2_clean_df = file_2_df.dropna(subset=[\"Latitude\", \"Longitude\"]).copy()"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Join 2 DataFrames\n",
   "id": "4a023ca96a3dee61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T05:24:08.015971Z",
     "start_time": "2025-04-02T05:24:07.967372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_1_subset = file_1_df[\n",
    "    [\"Borehole ID\",\n",
    "     \"Elevation (borehole_info: D)\",\n",
    "     \"Depth to Water (hydrologic: B)\"]\n",
    "]\n",
    "file_1_subset = file_1_subset.rename(columns={\"Elevation (borehole_info: D)\": \"Elevation\"})\n",
    "\n",
    "\n",
    "# Perform the merge\n",
    "merged_df = file_2_clean_df.merge(file_1_subset, on=\"Borehole ID\", how=\"left\")\n"
   ],
   "id": "d564aedb9184145e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T05:24:11.291873Z",
     "start_time": "2025-04-02T05:24:10.450479Z"
    }
   },
   "cell_type": "code",
   "source": "merged_df.to_csv('resources/borehole_logs.csv', index=False)",
   "id": "d4b3409481fba540",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T15:38:21.900303Z",
     "start_time": "2025-04-02T15:38:21.860278Z"
    }
   },
   "cell_type": "code",
   "source": "print(merged_df['Lithology'].unique())",
   "id": "11e4efc96ffd5f17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fill' 'Sandy silt' 'Silt' 'Topsoil / vegetation' 'Silty sand' 'Clay'\n",
      " 'Silty clay' 'Clayey silt' 'Clayey sand' 'Sand' 'Undefined' 'Gravel'\n",
      " 'Gravelly sand' 'Asphalt / concrete' 'Peat' 'Sandy gravel' 'Debris'\n",
      " 'Sandy clay' 'Silty gravel' 'Sedimentary bedrock' 'Gravelly silt'\n",
      " 'Gravelly clay' 'Undifferentiated rock' 'Clayey gravel'\n",
      " 'Cobbles / boulders' 'Plutonic bedrock' 'Volcanic ash' 'Volcanic bedrock'\n",
      " 'Metamorphic bedrock']\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "merge consecutive layers\n",
    "üìù Example\n",
    "Before:\n",
    "Borehole ID\tSub Borehole Layer\tLithology\tTop Depth\tBottom Depth\n",
    "1\t        4\t                Silt\t    3.0\t        4.0\n",
    "1\t        5\t                Silt\t    4.0\t        5.0\n",
    "\n",
    "After:\n",
    "Borehole ID\tSub Borehole Layer\tLithology\tTop Depth\tBottom Depth\n",
    "1\t            4\t            Silt\t    3.0\t        5.0"
   ],
   "id": "97f50f0f244c479e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T05:24:33.633739Z",
     "start_time": "2025-04-02T05:24:13.484697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('resources/borehole_logs.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Sort for proper processing\n",
    "df = df.sort_values(by=['Borehole ID', 'Sub Borehole Layer']).reset_index(drop=True)\n",
    "\n",
    "# Result list\n",
    "merged_rows = []\n",
    "\n",
    "# Group by Borehole ID\n",
    "for borehole_id, group in df.groupby('Borehole ID'):\n",
    "    group = group.sort_values(by='Sub Borehole Layer').reset_index(drop=True)\n",
    "    i = 0\n",
    "    while i < len(group):\n",
    "        current_row = group.iloc[i].copy()\n",
    "        j = i + 1\n",
    "        # Start checking consecutive rows\n",
    "        while (\n",
    "            j < len(group)\n",
    "            and group.loc[j, 'Lithology'] == current_row['Lithology']\n",
    "            and group.loc[j, 'Sub Borehole Layer'] == current_row['Sub Borehole Layer'] + (j - i)\n",
    "        ):\n",
    "            current_row['Bottom Depth'] = group.loc[j, 'Bottom Depth']\n",
    "            j += 1\n",
    "        # Set new Sub Borehole Layer (optional: store the range or lowest one)\n",
    "        current_row['Sub Borehole Layer'] = group.loc[i, 'Sub Borehole Layer']\n",
    "        merged_rows.append(current_row)\n",
    "        i = j\n",
    "\n",
    "# Final merged DataFrame\n",
    "merged_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "# Save if needed\n",
    "merged_df.to_csv(\"resources/merged_borehole_layers.csv\", index=False)\n",
    "\n",
    "print(\"Merged consecutive layers and saved to 'merged_borehole_layers.csv'\")\n"
   ],
   "id": "bfc6898b7bd2e0e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/z62syxvx173303z_ts3j44n40000gn/T/ipykernel_7840/2903188651.py:4: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('resources/borehole_logs.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged consecutive layers and saved to 'merged_borehole_layers.csv'\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T05:24:44.470414Z",
     "start_time": "2025-04-02T05:24:44.223971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and clean\n",
    "df = pd.read_csv('resources/merged_borehole_layers.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Group and count layers per borehole\n",
    "layer_counts = df.groupby('Borehole ID').size().reset_index(name='Layer Count')\n",
    "\n",
    "# Get top 5 boreholes with the most layers\n",
    "top_boreholes = layer_counts.sort_values(by='Layer Count', ascending=False).head()\n",
    "max_layers = layer_counts['Layer Count'].max()\n",
    "print(f\"Max number of layers in a single borehole: {max_layers}\")\n",
    "print(top_boreholes.head())\n",
    "\n",
    "# Filter original DataFrame\n",
    "filtered_df = df[df['Borehole ID'].isin(top_boreholes['Borehole ID'])]\n",
    "\n",
    "# Save to CSV\n",
    "filtered_df.to_csv(\"resources/top_borehole_records.csv\", index=False)\n",
    "print(\"Saved full records of top boreholes to 'top_borehole_records.csv'\")\n",
    "\n",
    "# Distribution bins\n",
    "bins = [0, 5, 10, 20, 50, 100]\n",
    "labels = ['1-5', '6-10', '11-20', '21-50', '51-100']\n",
    "layer_counts['Layer Range'] = pd.cut(layer_counts['Layer Count'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Count and percentage per group\n",
    "distribution = layer_counts['Layer Range'].value_counts().sort_index().reset_index()\n",
    "distribution.columns = ['Layer Range', 'Count']\n",
    "distribution['Percentage'] = (distribution['Count'] / distribution['Count'].sum() * 100).round(2)\n",
    "\n",
    "print(\"\\nLayer Count Distribution:\")\n",
    "print(distribution)"
   ],
   "id": "731f191d3f7b6d01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of layers in a single borehole: 76\n",
      "       Borehole ID  Layer Count\n",
      "63159        70329           76\n",
      "63158        70328           67\n",
      "63875        71903           65\n",
      "61591        68502           65\n",
      "63876        71904           63\n",
      "Saved full records of top boreholes to 'top_borehole_records.csv'\n",
      "\n",
      "Layer Count Distribution:\n",
      "  Layer Range  Count  Percentage\n",
      "0         1-5  79334       90.87\n",
      "1        6-10   6844        7.84\n",
      "2       11-20   1008        1.15\n",
      "3       21-50    113        0.13\n",
      "4      51-100      5        0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/z62syxvx173303z_ts3j44n40000gn/T/ipykernel_7840/1133239135.py:4: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('resources/merged_borehole_layers.csv')\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "‚úÖ Code to Keep Only Boreholes with 1‚Äì10 Layers:",
   "id": "89c8bf6d708dd93b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T05:24:51.075418Z",
     "start_time": "2025-04-02T05:24:50.224778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and clean\n",
    "df = pd.read_csv('resources/merged_borehole_layers.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Group and count layers per borehole\n",
    "layer_counts = df.groupby('Borehole ID').size().reset_index(name='Layer Count')\n",
    "\n",
    "# Filter to only 1-10 layers\n",
    "valid_ids = layer_counts[layer_counts['Layer Count'].between(1, 10)]['Borehole ID']\n",
    "\n",
    "# Filter the original DataFrame\n",
    "filtered_df = df[df['Borehole ID'].isin(valid_ids)]\n",
    "\n",
    "# Save or preview\n",
    "filtered_df.to_csv(\"resources/boreholes_1_to_10_layers.csv\", index=False)\n",
    "print(f\"Filtered boreholes with 1‚Äì10 layers: {filtered_df['Borehole ID'].nunique()} boreholes saved.\")\n"
   ],
   "id": "beef05d781d89a20",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/z62syxvx173303z_ts3j44n40000gn/T/ipykernel_7840/767122414.py:4: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('resources/merged_borehole_layers.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered boreholes with 1‚Äì10 layers: 86178 boreholes saved.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "‚úÖ Final Structure (Per Layer)\n",
    "\n",
    "üî¢ Inputs (Features):\n",
    "- Latitude\n",
    "- Longitude\n",
    "- Elevation\n",
    "\n",
    "üéØ Outputs:\n",
    "- Lithology ‚Üí classification\n",
    "- Top Depth ‚Üí regression\n",
    "- Bottom Depth ‚Üí regression\n",
    "\n"
   ],
   "id": "88ef0ac984e3cbff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T05:37:52.395506Z",
     "start_time": "2025-04-02T05:37:37.132023Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Borehole ID', 'Document ID', 'Sub Borehole Layer', 'Latitude', 'Longitude', 'Top Depth', 'Bottom Depth', 'Description', 'USCS', 'Lithology', 'Elevation', 'Depth to Water (hydrologic: B)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/z62syxvx173303z_ts3j44n40000gn/T/ipykernel_7840/2265567470.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  reshaped_df = df.groupby('Borehole ID').apply(reshape_borehole).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reshaped to wide format with up to 10 layers per borehole.\n"
     ]
    }
   ],
   "execution_count": 34,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"resources/boreholes_1_to_10_layers.csv\", low_memory=False)\n",
    "df.columns = df.columns.str.strip()  # Clean whitespace\n",
    "\n",
    "print(df.columns.tolist())  # Print all column names\n",
    "\n",
    "def reshape_borehole(group, max_layers=10):\n",
    "    row = {\n",
    "        'Borehole ID': group['Borehole ID'].iloc[0],\n",
    "        'Latitude': group['Latitude'].iloc[0],\n",
    "        'Longitude': group['Longitude'].iloc[0],\n",
    "        'Elevation': group['Elevation'].iloc[0],\n",
    "    }\n",
    "    group = group.sort_values(by='Top Depth')\n",
    "    for i in range(max_layers):\n",
    "        if i < len(group):\n",
    "            row[f'Lith_{i+1}'] = group['Lithology'].iloc[i]\n",
    "            row[f'Top_{i+1}'] = group['Top Depth'].iloc[i]\n",
    "            row[f'Bottom_{i+1}'] = group['Bottom Depth'].iloc[i]\n",
    "        else:\n",
    "            row[f'Lith_{i+1}'] = 'N/A'\n",
    "            row[f'Top_{i+1}'] = 0.0\n",
    "            row[f'Bottom_{i+1}'] = 0.0\n",
    "    return pd.Series(row)\n",
    "\n",
    "# Apply to full dataframe\n",
    "reshaped_df = df.groupby('Borehole ID').apply(reshape_borehole).reset_index(drop=True)\n",
    "\n",
    "# Save for reuse\n",
    "reshaped_df.to_csv(\"resources/boreholes_reshaped.csv\", index=False)\n",
    "print(\"‚úÖ Reshaped to wide format with up to 10 layers per borehole.\")"
   ],
   "id": "496c388b5dffbf9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "df1bf251ce5fa75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T07:05:44.009258Z",
     "start_time": "2025-04-02T06:59:22.585737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Add\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"resources/boreholes_reshaped.csv\")\n",
    "print(f\"Initial size: {df.shape}\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Drop rows with any missing Lithology, Top or Bottom depths\n",
    "# Replace None/NaN with \"N/A\" for Lithology, Top, and Bottom columns\n",
    "target_cols = [f'Lith_{i}' for i in range(1, 11)] + \\\n",
    "              [f'Top_{i}' for i in range(1, 11)] + \\\n",
    "              [f'Bottom_{i}' for i in range(1, 11)]\n",
    "\n",
    "df[target_cols] = df[target_cols].fillna(\"N/A\")\n",
    "\n",
    "# Input features\n",
    "X = df[['Latitude', 'Longitude', 'Elevation']].values\n",
    "print(f\"Initial X size: {df.shape}\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === Targets ===\n",
    "lith_label_encoders = {}\n",
    "y_lith_all = []\n",
    "\n",
    "# Fit on full dataframe to ensure consistent class count\n",
    "for i in range(1, 11):\n",
    "    col = f'Lith_{i}'\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].astype(str)\n",
    "    le.fit(df[col])  # Fit on full column\n",
    "    lith_label_encoders[col] = le\n",
    "    y_encoded = le.transform(df[col])\n",
    "    y_lith_all.append(to_categorical(y_encoded))\n",
    "\n",
    "\n",
    "# Top & Bottom Depths\n",
    "y_top_all = [df[f'Top_{i}'].values for i in range(1, 11)]\n",
    "y_bottom_all = [df[f'Bottom_{i}'].values for i in range(1, 11)]\n",
    "\n",
    "# === Train/Test split (80/20) ===\n",
    "X_train, X_test, df_train, df_test = train_test_split(X_scaled, df, test_size=0.2, random_state=42)\n",
    "print(f\"X_train size: {X_train.shape}\")\n",
    "\n",
    "y_lith_train = [\n",
    "    to_categorical(\n",
    "        lith_label_encoders[f'Lith_{i+1}'].transform(df_train[f'Lith_{i+1}'].astype(str)),\n",
    "        num_classes=len(lith_label_encoders[f'Lith_{i+1}'].classes_)\n",
    "    )\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "y_lith_test = [\n",
    "    to_categorical(\n",
    "        lith_label_encoders[f'Lith_{i+1}'].transform(df_test[f'Lith_{i+1}'].astype(str)),\n",
    "        num_classes=len(lith_label_encoders[f'Lith_{i+1}'].classes_)\n",
    "    )\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "y_top_train = [df_train[f'Top_{i+1}'].values for i in range(10)]\n",
    "y_top_test = [df_test[f'Top_{i+1}'].values for i in range(10)]\n",
    "\n",
    "y_bottom_train = [df_train[f'Bottom_{i+1}'].values for i in range(10)]\n",
    "y_bottom_test = [df_test[f'Bottom_{i+1}'].values for i in range(10)]\n",
    "\n",
    "# === Build Model ===\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "outputs = []\n",
    "\n",
    "def lithology_block(input_layer, i, num_classes):\n",
    "    x = Dense(128, activation='relu')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    return Dense(num_classes, activation='softmax', name=f'lith_{i}')(x)\n",
    "\n",
    "def top_block(input_layer, i):\n",
    "    x = Dense(64, activation='relu')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    return Dense(1, activation='linear', name=f'top_{i}')(x)\n",
    "\n",
    "def bottom_block(input_layer, i):\n",
    "    x = Dense(64, activation='relu')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    return Dense(1, activation='linear', name=f'bottom_{i}')(x)\n",
    "\n",
    "\n",
    "# Lithology classification heads\n",
    "for i in range(10):\n",
    "    num_classes = y_lith_train[i].shape[1]\n",
    "    outputs.append(lithology_block(input_layer, i + 1, num_classes))\n",
    "\n",
    "# Top and Bottom regression heads\n",
    "for i in range(10):\n",
    "    outputs.append(top_block(input_layer, i + 1))\n",
    "    outputs.append(bottom_block(input_layer, i + 1))\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "# Outputs\n",
    "outputs = []\n",
    "\n",
    "# Lithology (classification)\n",
    "for i in range(10):\n",
    "    num_classes = y_lith_train[i].shape[1]\n",
    "    outputs.append(lithology_block(input_layer, i + 1, num_classes))\n",
    "\n",
    "# Top and Bottom regression heads\n",
    "for i in range(10):\n",
    "    outputs.append(top_block(input_layer, i + 1))\n",
    "    outputs.append(bottom_block(input_layer, i + 1))\n",
    "\n",
    "# Compile\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "losses = {f'lith_{i+1}': 'categorical_crossentropy' for i in range(10)}\n",
    "losses.update({f'top_{i+1}': 'mse' for i in range(10)})\n",
    "losses.update({f'bottom_{i+1}': 'mse' for i in range(10)})\n",
    "\n",
    "metrics = {f'lith_{i+1}': 'accuracy' for i in range(10)}\n",
    "metrics.update({f'top_{i+1}': 'mae' for i in range(10)})\n",
    "metrics.update({f'bottom_{i+1}': 'mae' for i in range(10)})\n",
    "\n",
    "# Emphasize underperforming lith layers\n",
    "loss_weights = {\n",
    "    f'lith_1': 10.0,\n",
    "    f'lith_2': 5.0,\n",
    "    f'lith_3': 2.0,\n",
    "    f'lith_4': 1.0,\n",
    "    f'lith_5': 1.0,\n",
    "    f'lith_6': 0.8,\n",
    "    f'lith_7': 0.7,\n",
    "    f'lith_8': 0.6,\n",
    "    f'lith_9': 0.5,\n",
    "    f'lith_10': 0.5,\n",
    "}\n",
    "loss_weights.update({f'top_{i+1}': 1.0 for i in range(10)})\n",
    "loss_weights.update({f'bottom_{i+1}': 1.0 for i in range(10)})\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=losses,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# === Train ===\n",
    "class EpochPrinter(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_lith_train + y_top_train + y_bottom_train,\n",
    "    validation_data=(X_test, y_lith_test + y_top_test + y_bottom_test),\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    verbose=0,  # suppress built-in output\n",
    "    callbacks=[EpochPrinter()]\n",
    ")\n",
    "\n",
    "# === Save Model & Encoders ===\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model.save(\"models/borehole_multi_layer_model.keras\")\n",
    "\n",
    "with open(\"models/lithology_label_encoders.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lith_label_encoders, f)\n",
    "\n",
    "with open(\"models/feature_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"‚úÖ Model and encoders saved.\")\n",
    "\n",
    "\n"
   ],
   "id": "e4f8ed6d22ec13e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial size: (86178, 34)\n",
      "Initial X size: (86178, 34)\n",
      "X_train size: (68942, 3)\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n",
      "Epoch 51\n",
      "Epoch 52\n",
      "Epoch 53\n",
      "Epoch 54\n",
      "Epoch 55\n",
      "Epoch 56\n",
      "Epoch 57\n",
      "Epoch 58\n",
      "Epoch 59\n",
      "Epoch 60\n",
      "Epoch 61\n",
      "Epoch 62\n",
      "Epoch 63\n",
      "Epoch 64\n",
      "Epoch 65\n",
      "Epoch 66\n",
      "Epoch 67\n",
      "Epoch 68\n",
      "Epoch 69\n",
      "Epoch 70\n",
      "Epoch 71\n",
      "Epoch 72\n",
      "Epoch 73\n",
      "Epoch 74\n",
      "Epoch 75\n",
      "Epoch 76\n",
      "Epoch 77\n",
      "Epoch 78\n",
      "Epoch 79\n",
      "Epoch 80\n",
      "Epoch 81\n",
      "Epoch 82\n",
      "Epoch 83\n",
      "Epoch 84\n",
      "Epoch 85\n",
      "Epoch 86\n",
      "Epoch 87\n",
      "Epoch 88\n",
      "Epoch 89\n",
      "Epoch 90\n",
      "Epoch 91\n",
      "Epoch 92\n",
      "Epoch 93\n",
      "Epoch 94\n",
      "Epoch 95\n",
      "Epoch 96\n",
      "Epoch 97\n",
      "Epoch 98\n",
      "Epoch 99\n",
      "Epoch 100\n",
      "‚úÖ Model and encoders saved.\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T07:05:48.697670Z",
     "start_time": "2025-04-02T07:05:46.377645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# === Make Predictions ===\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"\\nüéØ Accuracy for Lithology:\")\n",
    "mismatches = []\n",
    "total_correct = 0\n",
    "total_count = 0\n",
    "\n",
    "for i in range(10):\n",
    "    y_pred = np.argmax(preds[i], axis=1)\n",
    "    y_true = np.argmax(y_lith_test[i], axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    correct = np.sum(y_pred == y_true)\n",
    "    count = len(y_true)\n",
    "\n",
    "    total_correct += correct\n",
    "    total_count += count\n",
    "\n",
    "    print(f\"  Lith_{i+1}: {acc * 100:.2f}%  ({correct}/{count})\")\n",
    "\n",
    "    # Collect mismatches for Lith_1 and Lith_2 only\n",
    "    if i < 2:\n",
    "        encoder = lith_label_encoders[f\"Lith_{i+1}\"]\n",
    "        y_true_labels = encoder.inverse_transform(y_true)\n",
    "        y_pred_labels = encoder.inverse_transform(y_pred)\n",
    "        for idx in np.where(y_true != y_pred)[0]:\n",
    "            mismatches.append((idx, f\"Lith_{i+1}\", y_true_labels[idx], y_pred_labels[idx]))\n",
    "\n",
    "# === Print up to 20 mismatches with real values\n",
    "print(\"\\n‚ùå Sample Lith_1 and Lith_2 Mismatches (up to 20):\")\n",
    "for idx, layer, actual, predicted in mismatches[:20]:\n",
    "    print(f\"  Test Index {idx} | {layer} | actual='{actual}', predicted='{predicted}'\")\n"
   ],
   "id": "4d8034ca550d353",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m539/539\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1ms/step\n",
      "\n",
      "üéØ Accuracy for Lithology:\n",
      "  Lith_1: 35.84%  (6178/17236)\n",
      "  Lith_2: 30.54%  (5264/17236)\n",
      "  Lith_3: 49.70%  (8566/17236)\n",
      "  Lith_4: 73.21%  (12619/17236)\n",
      "  Lith_5: 85.44%  (14726/17236)\n",
      "  Lith_6: 91.81%  (15825/17236)\n",
      "  Lith_7: 95.42%  (16446/17236)\n",
      "  Lith_8: 97.41%  (16790/17236)\n",
      "  Lith_9: 98.67%  (17006/17236)\n",
      "  Lith_10: 99.45%  (17142/17236)\n",
      "\n",
      "‚ùå Sample Lith_1 and Lith_2 Mismatches (up to 20):\n",
      "  Test Index 0 | Lith_1 | actual='Sandy silt', predicted='Silty sand'\n",
      "  Test Index 1 | Lith_1 | actual='Topsoil / vegetation', predicted='Silty sand'\n",
      "  Test Index 2 | Lith_1 | actual='Topsoil / vegetation', predicted='Silty sand'\n",
      "  Test Index 3 | Lith_1 | actual='Clayey silt', predicted='Silty sand'\n",
      "  Test Index 4 | Lith_1 | actual='Topsoil / vegetation', predicted='Silty sand'\n",
      "  Test Index 5 | Lith_1 | actual='Silty gravel', predicted='Silty sand'\n",
      "  Test Index 6 | Lith_1 | actual='Clayey silt', predicted='Silty sand'\n",
      "  Test Index 8 | Lith_1 | actual='Sandy silt', predicted='Silty sand'\n",
      "  Test Index 9 | Lith_1 | actual='Gravelly silt', predicted='Silty sand'\n",
      "  Test Index 10 | Lith_1 | actual='Sand', predicted='Silty sand'\n",
      "  Test Index 11 | Lith_1 | actual='Fill', predicted='Topsoil / vegetation'\n",
      "  Test Index 13 | Lith_1 | actual='Topsoil / vegetation', predicted='Silty sand'\n",
      "  Test Index 14 | Lith_1 | actual='Topsoil / vegetation', predicted='Silty sand'\n",
      "  Test Index 15 | Lith_1 | actual='Silty sand', predicted='Topsoil / vegetation'\n",
      "  Test Index 16 | Lith_1 | actual='Gravel', predicted='Silty sand'\n",
      "  Test Index 18 | Lith_1 | actual='Gravelly sand', predicted='Silty sand'\n",
      "  Test Index 19 | Lith_1 | actual='Gravel', predicted='Silty sand'\n",
      "  Test Index 20 | Lith_1 | actual='Clayey sand', predicted='Silty sand'\n",
      "  Test Index 21 | Lith_1 | actual='Topsoil / vegetation', predicted='Silty sand'\n",
      "  Test Index 22 | Lith_1 | actual='Topsoil / vegetation', predicted='Silty sand'\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inference Script: Predict 10 Layers",
   "id": "ea86e631c2a6dc3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T06:53:26.005411Z",
     "start_time": "2025-04-02T06:53:25.399729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# === Load model and saved encoders ===\n",
    "model = load_model(\"models/borehole_multi_layer_model.keras\")\n",
    "\n",
    "with open(\"models/lithology_label_encoders.pkl\", \"rb\") as f:\n",
    "    lith_encoders = pickle.load(f)\n",
    "\n",
    "with open(\"models/feature_scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# === Input a new borehole ===\n",
    "# Replace this with your real test input\n",
    "new_borehole = pd.DataFrame([{\n",
    "    'Latitude': 47.61,\n",
    "    'Longitude': -122.33,\n",
    "    'Elevation': 125.0\n",
    "}])\n",
    "\n",
    "# Scale input\n",
    "X_new_scaled = scaler.transform(new_borehole)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_new_scaled)\n",
    "\n",
    "# Parse predictions\n",
    "layer_results = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Lithology: decode from softmax\n",
    "    lith_pred_softmax = predictions[i][0]\n",
    "    lith_class = lith_encoders[f'Lith_{i+1}'].inverse_transform([np.argmax(lith_pred_softmax)])[0]\n",
    "\n",
    "    # Top Depth (regression output)\n",
    "    top_depth = predictions[10 + i][0][0]\n",
    "\n",
    "    # Bottom Depth (regression output)\n",
    "    bottom_depth = predictions[10 + i + 10][0][0]\n",
    "\n",
    "    layer_results.append({\n",
    "        'Layer': i + 1,\n",
    "        'Lithology': lith_class,\n",
    "        'Top Depth': round(top_depth, 2),\n",
    "        'Bottom Depth': round(bottom_depth, 2)\n",
    "    })\n",
    "\n",
    "# === Display Result ===\n",
    "result_df = pd.DataFrame(layer_results)\n",
    "print(\"\\nüîÆ Predicted Soil Layers:\")\n",
    "print(result_df.to_string(index=False))\n"
   ],
   "id": "bc7be973d496d3a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anatolykuznetsov/PycharmProjects/GemPy model/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 270ms/step\n",
      "\n",
      "üîÆ Predicted Soil Layers:\n",
      " Layer          Lithology  Top Depth  Bottom Depth\n",
      "     1 Asphalt / concrete       0.01      7.250000\n",
      "     2         Silty sand       5.63     13.430000\n",
      "     3                N/A      11.36     20.209999\n",
      "     4                N/A      13.29     19.760000\n",
      "     5                N/A      16.09     23.120001\n",
      "     6                N/A      15.80     19.480000\n",
      "     7                N/A      12.74     14.810000\n",
      "     8                N/A       8.22     10.460000\n",
      "     9                N/A       4.33      5.290000\n",
      "    10                N/A       2.19      2.240000\n"
     ]
    }
   ],
   "execution_count": 73
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
